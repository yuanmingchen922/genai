{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1913f38",
   "metadata": {},
   "source": [
    "# Assignment 4: Advanced Image Generation\n",
    "## Diffusion Models and Energy-Based Models\n",
    "\n",
    "**Student**: my2878  \n",
    "**Course**: Generative AI  \n",
    "**Date**: November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements two advanced generative models:\n",
    "\n",
    "1. **Diffusion Model (DDPM)** - Denoising Diffusion Probabilistic Model\n",
    "2. **Energy-Based Model (EBM)** - Using Langevin Dynamics\n",
    "\n",
    "Both models are trained on **CIFAR-10** dataset and integrated into the FastAPI.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Part 1: Diffusion Model Implementation](#diffusion)\n",
    "3. [Part 2: Energy-Based Model Implementation](#energy)\n",
    "4. [Part 3: Training on CIFAR-10](#training)\n",
    "5. [Part 4: Theory Questions](#theory)\n",
    "6. [Part 5: Results and Visualization](#results)\n",
    "7. [Part 6: API Integration](#api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4674603e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e8a9c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \n",
    "                     'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba07b30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Part 1: Diffusion Model Implementation\n",
    "\n",
    "### 2.1 Sinusoidal Time Embedding\n",
    "\n",
    "The sinusoidal time embedding provides a continuous representation of timesteps using sine and cosine functions.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "\n",
    "For timestep $t$ and embedding dimension $d$, the $i$-th dimension is:\n",
    "\n",
    "$$\n",
    "\\text{embedding}[2i] = \\sin\\left(\\frac{t}{10000^{2i/d}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{embedding}[2i+1] = \\cos\\left(\\frac{t}{10000^{2i/d}}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab34c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Embedding for t=1, d=8:\n",
      "[0.84147096 0.09983341 0.00999983 0.001      0.54030234 0.9950042\n",
      " 0.99995    0.9999995 ]\n",
      "\n",
      "This matches our theoretical calculation!\n"
     ]
    }
   ],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Time Embedding for diffusion timesteps.\n",
    "    \n",
    "    This provides a continuous, deterministic embedding for each timestep\n",
    "    using sine and cosine functions at different frequencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=128, max_period=10000):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_period = max_period\n",
    "    \n",
    "    def forward(self, timesteps):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            timesteps: (batch_size,) tensor of timestep indices\n",
    "        \n",
    "        Returns:\n",
    "            (batch_size, embedding_dim) tensor of embeddings\n",
    "        \"\"\"\n",
    "        device = timesteps.device\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        \n",
    "        # Calculate frequency scaling\n",
    "        frequencies = torch.exp(\n",
    "            -math.log(self.max_period) * torch.arange(half_dim, device=device) / half_dim\n",
    "        )\n",
    "        \n",
    "        # Compute arguments: t * frequency\n",
    "        args = timesteps[:, None].float() * frequencies[None, :]\n",
    "        \n",
    "        # Concatenate sin and cos components\n",
    "        embedding = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "# Test the time embedding\n",
    "time_embed = SinusoidalTimeEmbedding(embedding_dim=8, max_period=10000)\n",
    "t = torch.tensor([1])\n",
    "embedding = time_embed(t)\n",
    "print(\"Time Embedding for t=1, d=8:\")\n",
    "print(embedding.numpy()[0])\n",
    "print(\"\\nThis matches our theoretical calculation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a312cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f2dd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ UNet building blocks defined\n"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with time embedding injection.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, time_emb_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.residual_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.residual_conv = nn.Identity()\n",
    "    \n",
    "    def forward(self, x, time_emb):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Inject time embedding\n",
    "        time_emb = self.time_mlp(F.relu(time_emb))\n",
    "        x = x + time_emb[:, :, None, None]\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x + self.residual_conv(residual)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Self-attention block for spatial features.\"\"\"\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(8, channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        residual = x\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=1)\n",
    "        \n",
    "        q = q.reshape(B, C, H * W).permute(0, 2, 1)\n",
    "        k = k.reshape(B, C, H * W)\n",
    "        v = v.reshape(B, C, H * W).permute(0, 2, 1)\n",
    "        \n",
    "        attn = torch.bmm(q, k) / math.sqrt(C)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.bmm(attn, v)\n",
    "        out = out.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out + residual\n",
    "\n",
    "print(\"✓ UNet building blocks defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c27cb",
   "metadata": {},
   "source": [
    "### 2.3 Complete UNet Architecture\n",
    "\n",
    "The UNet predicts the noise $\\epsilon$ that was added to create the noisy image at timestep $t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81fd541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ UNet created with 7,719,747 parameters\n",
      "✓ Forward pass successful: torch.Size([2, 3, 32, 32]) -> torch.Size([2, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Simplified UNet for CIFAR-10 (32x32 images)\n",
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"Simplified UNet for CIFAR-10 diffusion model.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=3, out_channels=3, time_emb_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalTimeEmbedding(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_emb_dim * 4, time_emb_dim * 4)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.init_conv = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.down1 = ResidualBlock(64, 128, time_emb_dim * 4)\n",
    "        self.down2 = ResidualBlock(128, 256, time_emb_dim * 4)\n",
    "        self.downsample1 = nn.Conv2d(128, 128, 3, stride=2, padding=1)\n",
    "        self.downsample2 = nn.Conv2d(256, 256, 3, stride=2, padding=1)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.mid1 = ResidualBlock(256, 256, time_emb_dim * 4)\n",
    "        self.mid_attn = AttentionBlock(256)\n",
    "        self.mid2 = ResidualBlock(256, 256, time_emb_dim * 4)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = ResidualBlock(512, 128, time_emb_dim * 4)  # 256 + 256\n",
    "        self.up2 = ResidualBlock(256, 64, time_emb_dim * 4)   # 128 + 128\n",
    "        self.upsample1 = nn.ConvTranspose2d(256, 256, 4, stride=2, padding=1)\n",
    "        self.upsample2 = nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1)\n",
    "        \n",
    "        # Output\n",
    "        self.final_norm = nn.GroupNorm(8, 64)\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, 3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t)\n",
    "        \n",
    "        # Initial conv\n",
    "        x = self.init_conv(x)\n",
    "        skip0 = x\n",
    "        \n",
    "        # Encoder\n",
    "        x = self.down1(x, t_emb)\n",
    "        skip1 = x\n",
    "        x = self.downsample1(x)\n",
    "        \n",
    "        x = self.down2(x, t_emb)\n",
    "        skip2 = x\n",
    "        x = self.downsample2(x)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.mid1(x, t_emb)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid2(x, t_emb)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.upsample1(x)\n",
    "        x = torch.cat([x, skip2], dim=1)\n",
    "        x = self.up1(x, t_emb)\n",
    "        \n",
    "        x = self.upsample2(x)\n",
    "        x = torch.cat([x, skip1], dim=1)\n",
    "        x = self.up2(x, t_emb)\n",
    "        \n",
    "        # Output\n",
    "        x = self.final_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test UNet\n",
    "unet = SimpleUNet().to(device)\n",
    "num_params = sum(p.numel() for p in unet.parameters())\n",
    "print(f\"✓ UNet created with {num_params:,} parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "test_x = torch.randn(2, 3, 32, 32).to(device)\n",
    "test_t = torch.tensor([0, 100]).to(device)\n",
    "test_out = unet(test_x, test_t)\n",
    "print(f\"✓ Forward pass successful: {test_x.shape} -> {test_out.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d5f6f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Part 2: Theory Questions - Building Blocks of Energy Model\n",
    "\n",
    "### Question 6: Basic Gradient Calculations\n",
    "\n",
    "Understanding how PyTorch tracks gradients through computational graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc02abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Question 6: Basic Gradient Calculations\n",
      "======================================================================\n",
      "\n",
      "Original code result:\n",
      "x.grad = tensor([7.])\n",
      "\n",
      "Expected: dy/dx = 2x + 3 = 2(2) + 3 = 7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Question 6: Basic Gradient Calculations\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a tensor with requires_grad=True\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Define a simple function y = x² + 3x\n",
    "y = x**2 + 3 * x\n",
    "\n",
    "# Backpropagate\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient\n",
    "print(\"\\nOriginal code result:\")\n",
    "print(\"x.grad =\", x.grad)\n",
    "print(\"\\nExpected: dy/dx = 2x + 3 = 2(2) + 3 = 7\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ce199",
   "metadata": {},
   "source": [
    "#### Answer 6a: Expected Gradient\n",
    "\n",
    "For $y = x^2 + 3x$, the gradient is:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = 2x + 3\n",
    "$$\n",
    "\n",
    "At $x = 2$:\n",
    "$$\n",
    "\\frac{dy}{dx}\\bigg|_{x=2} = 2(2) + 3 = 4 + 3 = 7\n",
    "$$\n",
    "\n",
    "**Expected gradient: 7.0**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "526557f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Answer 6b: Setting requires_grad=False\n",
      "======================================================================\n",
      "\n",
      "Error occurred: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "\n",
      "Explanation: When requires_grad=False, PyTorch does not build\n",
      "a computational graph, so .backward() fails because gradients\n",
      "cannot be computed. The tensor y_no_grad also has requires_grad=False.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Answer 6b: Setting requires_grad=False\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create tensor with requires_grad=False\n",
    "x_no_grad = torch.tensor([2.0], requires_grad=False)\n",
    "y_no_grad = x_no_grad**2 + 3 * x_no_grad\n",
    "\n",
    "try:\n",
    "    y_no_grad.backward()\n",
    "    print(\"x_no_grad.grad =\", x_no_grad.grad)\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nError occurred: {e}\")\n",
    "    print(\"\\nExplanation: When requires_grad=False, PyTorch does not build\")\n",
    "    print(\"a computational graph, so .backward() fails because gradients\")\n",
    "    print(\"cannot be computed. The tensor y_no_grad also has requires_grad=False.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "963004a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Answer 6c: Default behavior of torch.tensor\n",
      "======================================================================\n",
      "\n",
      "x_default.requires_grad: False\n",
      "y_default.requires_grad: False\n",
      "\n",
      "Explanation:\n",
      "By default, torch.tensor() creates tensors with requires_grad=False.\n",
      "From PyTorch documentation:\n",
      "  'requires_grad (bool, optional) – If autograd should record operations\n",
      "   on the returned tensor. Default: False.'\n",
      "\n",
      "To enable gradient tracking, you must explicitly set requires_grad=True.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Answer 6c: Default behavior of torch.tensor\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create tensor without specifying requires_grad\n",
    "x_default = torch.tensor([2.0])\n",
    "y_default = x_default**2 + 3 * x_default\n",
    "\n",
    "print(f\"\\nx_default.requires_grad: {x_default.requires_grad}\")\n",
    "print(f\"y_default.requires_grad: {y_default.requires_grad}\")\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"By default, torch.tensor() creates tensors with requires_grad=False.\")\n",
    "print(\"From PyTorch documentation:\")\n",
    "print(\"  'requires_grad (bool, optional) – If autograd should record operations\")\n",
    "print(\"   on the returned tensor. Default: False.'\")\n",
    "print(\"\\nTo enable gradient tracking, you must explicitly set requires_grad=True.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c250de5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 7: Introduce Weights\n",
    "\n",
    "When training neural networks, we're interested in gradients with respect to model weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d29b8335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question 7: Introduce Weights\n",
      "======================================================================\n",
      "\n",
      "Gradient with respect to x:\n",
      "x.grad = tensor([7.])\n",
      "\n",
      "======================================================================\n",
      "Answer 7a: Gradient with respect to w\n",
      "======================================================================\n",
      "\n",
      "Attempting to access w.grad:\n",
      "w.grad = None\n",
      "\n",
      "Explanation:\n",
      "w.grad is None because w was created without requires_grad=True.\n",
      "By default, torch.tensor() sets requires_grad=False.\n",
      "PyTorch only computes and stores gradients for tensors that have\n",
      "requires_grad=True.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Question 7: Introduce Weights\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a tensor with requires_grad=True\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "w = torch.tensor([1.0, 3.0])\n",
    "\n",
    "# Define a simple function y = x² + 3x\n",
    "y = w[0] * x**2 + w[1] * x\n",
    "\n",
    "# Backpropagate\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient\n",
    "print(\"\\nGradient with respect to x:\")\n",
    "print(\"x.grad =\", x.grad)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Answer 7a: Gradient with respect to w\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nAttempting to access w.grad:\")\n",
    "print(\"w.grad =\", w.grad)\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"w.grad is None because w was created without requires_grad=True.\")\n",
    "print(\"By default, torch.tensor() sets requires_grad=False.\")\n",
    "print(\"PyTorch only computes and stores gradients for tensors that have\")\n",
    "print(\"requires_grad=True.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c17766",
   "metadata": {},
   "source": [
    "#### Answer 7b: Modified Code to Calculate Gradient w.r.t. w\n",
    "\n",
    "To compute gradients with respect to $w$, we need to set `requires_grad=True` for $w$.\n",
    "\n",
    "Mathematical derivation:\n",
    "$$\n",
    "y = w_0 \\cdot x^2 + w_1 \\cdot x\n",
    "$$\n",
    "\n",
    "Gradients:\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial w_0} = x^2 = 2^2 = 4\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial w_1} = x = 2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a44806f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Answer 7b: Modified Code - Computing w.grad\n",
      "======================================================================\n",
      "\n",
      "Gradients:\n",
      "x_new.grad = tensor([7.])\n",
      "w_new.grad = tensor([4., 2.])\n",
      "\n",
      "Verification:\n",
      "∂y/∂w[0] = x² = 2² = 4.0 ✓\n",
      "∂y/∂w[1] = x = 2.0 ✓\n",
      "∂y/∂x = 2·w[0]·x + w[1] = 2·1·2 + 3 = 7.0 ✓\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Answer 7b: Modified Code - Computing w.grad\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create tensors with requires_grad=True\n",
    "x_new = torch.tensor([2.0], requires_grad=True)\n",
    "w_new = torch.tensor([1.0, 3.0], requires_grad=True)  # Enable gradient tracking for w\n",
    "\n",
    "# Define the function\n",
    "y_new = w_new[0] * x_new**2 + w_new[1] * x_new\n",
    "\n",
    "# Backpropagate\n",
    "y_new.backward()\n",
    "\n",
    "# Print gradients\n",
    "print(\"\\nGradients:\")\n",
    "print(\"x_new.grad =\", x_new.grad)\n",
    "print(\"w_new.grad =\", w_new.grad)\n",
    "\n",
    "print(\"\\nVerification:\")\n",
    "print(\"∂y/∂w[0] = x² = 2² = 4.0 ✓\")\n",
    "print(\"∂y/∂w[1] = x = 2.0 ✓\")\n",
    "print(\"∂y/∂x = 2·w[0]·x + w[1] = 2·1·2 + 3 = 7.0 ✓\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7979e4a",
   "metadata": {},
   "source": [
    "#### Answer 7c: Default Gradient Tracking\n",
    "\n",
    "Same as Question 6c: **No, gradients are NOT tracked by default.**\n",
    "\n",
    "According to PyTorch documentation for `torch.tensor()`:\n",
    "- Default value for `requires_grad` is `False`\n",
    "- You must explicitly set `requires_grad=True` to enable automatic differentiation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8a973",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 8: Breaking the Graph\n",
    "\n",
    "Understanding how `.detach()` breaks the computational graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f9999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question 8: Breaking the Graph\n",
      "======================================================================\n",
      "\n",
      "Attempting to call w.backward()...\n",
      "Error: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Why does this fail?\n",
      "----------------------------------------------------------------------\n",
      "1. x has requires_grad=True\n",
      "2. y = x * 3 creates a node in the computational graph\n",
      "3. z = y.detach() creates a NEW tensor that shares data with y\n",
      "   but is DETACHED from the computational graph\n",
      "4. w = z * 2 operates on the detached tensor z\n",
      "5. w has no connection to x in the graph, so w.requires_grad=False\n",
      "6. Calling backward() on a tensor with requires_grad=False fails\n",
      "\n",
      "Tensor properties:\n",
      "  x.requires_grad: True\n",
      "  y.requires_grad: True\n",
      "  z.requires_grad: False\n",
      "  w.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Question 8: Breaking the Graph\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Original code that fails\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = x * 3\n",
    "z = y.detach()  # This breaks the computational graph\n",
    "w = z * 2\n",
    "\n",
    "print(\"\\nAttempting to call w.backward()...\")\n",
    "try:\n",
    "    w.backward()\n",
    "    print(\"Success! w.grad =\", w.grad)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Why does this fail?\")\n",
    "print(\"-\" * 70)\n",
    "print(\"1. x has requires_grad=True\")\n",
    "print(\"2. y = x * 3 creates a node in the computational graph\")\n",
    "print(\"3. z = y.detach() creates a NEW tensor that shares data with y\")\n",
    "print(\"   but is DETACHED from the computational graph\")\n",
    "print(\"4. w = z * 2 operates on the detached tensor z\")\n",
    "print(\"5. w has no connection to x in the graph, so w.requires_grad=False\")\n",
    "print(\"6. Calling backward() on a tensor with requires_grad=False fails\")\n",
    "\n",
    "print(f\"\\nTensor properties:\")\n",
    "print(f\"  x.requires_grad: {x.requires_grad}\")\n",
    "print(f\"  y.requires_grad: {y.requires_grad}\")\n",
    "print(f\"  z.requires_grad: {z.requires_grad}\")\n",
    "print(f\"  w.requires_grad: {w.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c599d5",
   "metadata": {},
   "source": [
    "#### Fixed Code - Solution\n",
    "\n",
    "To fix this while still using `z`, we need to keep it connected to the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a42270d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Fixed Code - Solution\n",
      "======================================================================\n",
      "\n",
      "Calling w.backward()...\n",
      "Success! x.grad = tensor([6.])\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Verification:\n",
      "----------------------------------------------------------------------\n",
      "w = z * 2 = (y) * 2 = (x * 3) * 2 = x * 6\n",
      "dw/dx = 6\n",
      "Computed gradient: 6.0 ✓\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Alternative Solutions:\n",
      "----------------------------------------------------------------------\n",
      "1. Don't use detach() at all (shown above)\n",
      "2. If you NEED to use z for non-gradient operations,\n",
      "   keep a separate variable for the graph:\n",
      "   z_detached = y.detach()  # For non-gradient ops\n",
      "   w = y * 2  # For gradient ops\n",
      "3. Use torch.no_grad() context for operations that\n",
      "   don't need gradients, but keep the main path intact\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Fixed Code - Solution\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Solution: Don't detach z\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = x * 3\n",
    "z = y  # Don't detach - keep the graph connection\n",
    "w = z * 2\n",
    "\n",
    "print(\"\\nCalling w.backward()...\")\n",
    "w.backward()\n",
    "\n",
    "print(f\"Success! x.grad = {x.grad}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Verification:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"w = z * 2 = (y) * 2 = (x * 3) * 2 = x * 6\")\n",
    "print(\"dw/dx = 6\")\n",
    "print(f\"Computed gradient: {x.grad.item()} ✓\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Alternative Solutions:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"1. Don't use detach() at all (shown above)\")\n",
    "print(\"2. If you NEED to use z for non-gradient operations,\")\n",
    "print(\"   keep a separate variable for the graph:\")\n",
    "print(\"   z_detached = y.detach()  # For non-gradient ops\")\n",
    "print(\"   w = y * 2  # For gradient ops\")\n",
    "print(\"3. Use torch.no_grad() context for operations that\")\n",
    "print(\"   don't need gradients, but keep the main path intact\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d0e07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 9: Gradient Accumulation\n",
    "\n",
    "Understanding how gradients accumulate in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34c4ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question 9: Gradient Accumulation\n",
      "======================================================================\n",
      "After first backward: x.grad = tensor([2.])\n",
      "After second backward: x.grad = tensor([5.])\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "What is happening?\n",
      "----------------------------------------------------------------------\n",
      "PyTorch ACCUMULATES gradients by default!\n",
      "First backward:  x.grad = dy1/dx = 2\n",
      "Second backward: x.grad = 2 + dy2/dx = 2 + 3 = 5\n",
      "\n",
      "This is useful for:\n",
      "  - Gradient accumulation across mini-batches\n",
      "  - Multi-task learning\n",
      "  - RNNs with multiple time steps\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Question 9: Gradient Accumulation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# First backward\n",
    "y1 = x * 2\n",
    "y1.backward()\n",
    "print(f\"After first backward: x.grad = {x.grad}\")\n",
    "\n",
    "# Second backward\n",
    "y2 = x * 3\n",
    "y2.backward()\n",
    "print(f\"After second backward: x.grad = {x.grad}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"What is happening?\")\n",
    "print(\"-\" * 70)\n",
    "print(\"PyTorch ACCUMULATES gradients by default!\")\n",
    "print(\"First backward:  x.grad = dy1/dx = 2\")\n",
    "print(\"Second backward: x.grad = 2 + dy2/dx = 2 + 3 = 5\")\n",
    "print(\"\\nThis is useful for:\")\n",
    "print(\"  - Gradient accumulation across mini-batches\")\n",
    "print(\"  - Multi-task learning\")\n",
    "print(\"  - RNNs with multiple time steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e8e5c",
   "metadata": {},
   "source": [
    "#### Solution: Zero the Gradients\n",
    "\n",
    "To avoid unwanted accumulation, call `.zero_()` or set `.grad = None` between backward passes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14078454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Solution: Proper Gradient Management\n",
      "======================================================================\n",
      "\n",
      "Method 1: Using x.grad.zero_()\n",
      "----------------------------------------------------------------------\n",
      "After first backward: x1.grad = tensor([2.])\n",
      "After second backward (zeroed): x1.grad = tensor([3.])\n",
      "\n",
      "Method 2: Setting x.grad = None\n",
      "----------------------------------------------------------------------\n",
      "After first backward: x2.grad = tensor([2.])\n",
      "After second backward (reset): x2.grad = tensor([3.])\n",
      "\n",
      "Method 3: Using optimizer.zero_grad() (typical in training)\n",
      "----------------------------------------------------------------------\n",
      "After first backward: x3.grad = tensor([2.])\n",
      "After second backward (optimizer zeroed): x3.grad = tensor([3.])\n",
      "\n",
      "======================================================================\n",
      "Best Practices:\n",
      "======================================================================\n",
      "1. In training loops, call optimizer.zero_grad() before each\n",
      "   backward pass\n",
      "2. Use x.grad = None (more efficient) or x.grad.zero_()\n",
      "3. Only accumulate gradients intentionally (e.g., for gradient\n",
      "   accumulation across mini-batches)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Solution: Proper Gradient Management\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Method 1: Using zero_()\n",
    "print(\"\\nMethod 1: Using x.grad.zero_()\")\n",
    "print(\"-\" * 70)\n",
    "x1 = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "y1 = x1 * 2\n",
    "y1.backward()\n",
    "print(f\"After first backward: x1.grad = {x1.grad}\")\n",
    "\n",
    "x1.grad.zero_()  # Zero the gradients\n",
    "y2 = x1 * 3\n",
    "y2.backward()\n",
    "print(f\"After second backward (zeroed): x1.grad = {x1.grad}\")\n",
    "\n",
    "# Method 2: Setting grad to None\n",
    "print(\"\\nMethod 2: Setting x.grad = None\")\n",
    "print(\"-\" * 70)\n",
    "x2 = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "y1 = x2 * 2\n",
    "y1.backward()\n",
    "print(f\"After first backward: x2.grad = {x2.grad}\")\n",
    "\n",
    "x2.grad = None  # Reset gradients\n",
    "y2 = x2 * 3\n",
    "y2.backward()\n",
    "print(f\"After second backward (reset): x2.grad = {x2.grad}\")\n",
    "\n",
    "# Method 3: Using optimizer (typical in training)\n",
    "print(\"\\nMethod 3: Using optimizer.zero_grad() (typical in training)\")\n",
    "print(\"-\" * 70)\n",
    "x3 = torch.tensor([1.0], requires_grad=True)\n",
    "optimizer = torch.optim.SGD([x3], lr=0.01)\n",
    "\n",
    "y1 = x3 * 2\n",
    "y1.backward()\n",
    "print(f\"After first backward: x3.grad = {x3.grad}\")\n",
    "\n",
    "optimizer.zero_grad()  # Zero all gradients managed by optimizer\n",
    "y2 = x3 * 3\n",
    "y2.backward()\n",
    "print(f\"After second backward (optimizer zeroed): x3.grad = {x3.grad}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Best Practices:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. In training loops, call optimizer.zero_grad() before each\")\n",
    "print(\"   backward pass\")\n",
    "print(\"2. Use x.grad = None (more efficient) or x.grad.zero_()\")\n",
    "print(\"3. Only accumulate gradients intentionally (e.g., for gradient\")\n",
    "print(\"   accumulation across mini-batches)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe4d63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary of Gradient Computation Questions\n",
    "\n",
    "### Key Learnings:\n",
    "\n",
    "1. **Question 6**: Basic gradient calculation - PyTorch autograd fundamentals\n",
    "2. **Question 7**: Weight gradients - Must set `requires_grad=True` for learnable parameters\n",
    "3. **Question 8**: Graph connectivity - `.detach()` breaks the computational graph\n",
    "4. **Question 9**: Gradient accumulation - Gradients accumulate by default, must zero them\n",
    "\n",
    "### Important Points:\n",
    "\n",
    "- **Default behavior**: `torch.tensor()` has `requires_grad=False` by default\n",
    "- **Computational graph**: Only built for tensors with `requires_grad=True`\n",
    "- **`.detach()`**: Creates a new tensor that shares data but has no gradient history\n",
    "- **Gradient accumulation**: Intentional feature, useful for mini-batch accumulation\n",
    "- **Best practice**: Always call `optimizer.zero_grad()` in training loops\n",
    "\n",
    "These concepts are essential for understanding:\n",
    "- How Energy-Based Models compute gradients for Langevin sampling\n",
    "- How training loops work in PyTorch\n",
    "- How to debug gradient flow issues\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
