{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737106c5",
   "metadata": {},
   "source": [
    "# Assignment 5: Post-Training LLM with Fine-tuning\n",
    "\n",
    "**Student ID**: my2878\n",
    "\n",
    "## Objective\n",
    "\n",
    "Fine-tune a GPT2 model (`openai-community/gpt2`) on the SQuAD dataset to generate responses in a specific format:\n",
    "- **Prefix**: \"That is a great question! \"\n",
    "- **Suffix**: \" Let me know if you have any other questions.\"\n",
    "\n",
    "This notebook contains:\n",
    "1. Understanding of RL concepts from Module 10 & 11\n",
    "2. Implementation of GPT2 fine-tuning\n",
    "3. API integration for text generation\n",
    "4. Testing and evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ba911",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Reinforcement Learning Concepts Review\n",
    "\n",
    "### Key RL Terminology\n",
    "\n",
    "| Term | Definition | Application to LLM |\n",
    "|------|------------|-------------------|\n",
    "| **Agent** | Decision-making entity | The language model |\n",
    "| **State** | Current situation representation | Current context/tokens |\n",
    "| **Action** | Choice made by agent | Next token selection |\n",
    "| **Reward** | Feedback signal | Response quality score |\n",
    "| **Policy** | Strategy for choosing actions | Token probability distribution |\n",
    "| **Value Function** | Expected cumulative reward | Expected response quality |\n",
    "\n",
    "### How RL Applies to LLM Fine-tuning\n",
    "\n",
    "In the context of LLM post-training:\n",
    "\n",
    "1. **State**: The prompt/question + generated tokens so far\n",
    "2. **Action**: Selecting the next token from vocabulary\n",
    "3. **Reward**: Can be shaped to encourage specific behaviors:\n",
    "   - Format compliance (starting/ending with specific phrases)\n",
    "   - Answer quality (relevance, accuracy)\n",
    "   - Response length (appropriate verbosity)\n",
    "\n",
    "### Reward Shaping for Custom Format\n",
    "\n",
    "Our reward function encourages:\n",
    "- Starting with \"That is a great question!\"\n",
    "- Ending with \"Let me know if you have any other questions.\"\n",
    "- Providing relevant answers from context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7df339",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Setup and Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Check device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac28721",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Response Format Definition\n",
    "\n",
    "We define our custom response format that the model will learn to generate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e3ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response format constants\n",
    "RESPONSE_PREFIX = \"That is a great question! \"\n",
    "RESPONSE_SUFFIX = \" Let me know if you have any other questions.\"\n",
    "\n",
    "def format_training_example(question: str, answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Format a QA pair for training with custom response format.\n",
    "    \n",
    "    Args:\n",
    "        question: The input question\n",
    "        answer: The answer to the question\n",
    "    \n",
    "    Returns:\n",
    "        Formatted training string\n",
    "    \"\"\"\n",
    "    return (\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer: {RESPONSE_PREFIX}{answer}{RESPONSE_SUFFIX}\"\n",
    "    )\n",
    "\n",
    "# Example\n",
    "example_q = \"What is machine learning?\"\n",
    "example_a = \"Machine learning is a subset of AI that enables systems to learn from data.\"\n",
    "print(\"Formatted Example:\")\n",
    "print(format_training_example(example_q, example_a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2920a4fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Load SQuAD Dataset\n",
    "\n",
    "We use the Stanford Question Answering Dataset (SQuAD) from HuggingFace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fa2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SQuAD dataset\n",
    "print(\"Loading SQuAD dataset...\")\n",
    "squad_dataset = load_dataset(\"rajpurkar/squad\")\n",
    "\n",
    "print(f\"Train examples: {len(squad_dataset['train'])}\")\n",
    "print(f\"Validation examples: {len(squad_dataset['validation'])}\")\n",
    "\n",
    "# Show sample\n",
    "sample = squad_dataset['train'][0]\n",
    "print(\"\\nSample entry:\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Answer: {sample['answers']['text'][0]}\")\n",
    "print(f\"Context: {sample['context'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2297686",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Custom Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuADDataset(Dataset):\n",
    "    \"\"\"Custom dataset for SQuAD with formatted responses.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, split=\"train\", max_length=256, max_samples=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"Loading SQuAD {split} dataset...\")\n",
    "        dataset = load_dataset(\"rajpurkar/squad\", split=split)\n",
    "        \n",
    "        if max_samples:\n",
    "            dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "        \n",
    "        self.examples = []\n",
    "        \n",
    "        print(\"Formatting examples...\")\n",
    "        for item in tqdm(dataset, desc=\"Processing\"):\n",
    "            question = item[\"question\"]\n",
    "            answer = item[\"answers\"][\"text\"][0] if item[\"answers\"][\"text\"] else \"\"\n",
    "            \n",
    "            if answer:\n",
    "                formatted = format_training_example(question, answer)\n",
    "                self.examples.append(formatted)\n",
    "        \n",
    "        print(f\"Loaded {len(self.examples)} examples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.examples[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        labels = input_ids.clone()\n",
    "        labels[attention_mask == 0] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec11f0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Load GPT2 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1decab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT2 model and tokenizer\n",
    "model_name = \"openai-community/gpt2\"\n",
    "\n",
    "print(f\"Loading tokenizer from {model_name}...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token (GPT2 doesn't have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loading model from {model_name}...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "# Print model info\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Total parameters: {num_params:,}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f9b8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f14564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "EPOCHS = 1  # Use more epochs for better results (3-5 recommended)\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_LENGTH = 256\n",
    "MAX_SAMPLES = 1000  # Use None for full dataset\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"../models/gpt2_finetuned\"\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"  Max samples: {MAX_SAMPLES}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a77af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt2(model, tokenizer, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "               learning_rate=LEARNING_RATE, max_samples=MAX_SAMPLES, output_dir=OUTPUT_DIR):\n",
    "    \"\"\"Fine-tune GPT2 on SQuAD dataset.\"\"\"\n",
    "    \n",
    "    # Create dataset\n",
    "    train_dataset = SQuADDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        split=\"train\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        max_samples=max_samples\n",
    "    )\n",
    "    \n",
    "    # Create dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nStarting training...\")\n",
    "    model.train()\n",
    "    training_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            epoch_loss += loss.item()\n",
    "            training_losses.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} completed. Average loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"model_name\": model_name,\n",
    "        \"is_fine_tuned\": True,\n",
    "        \"response_prefix\": RESPONSE_PREFIX,\n",
    "        \"response_suffix\": RESPONSE_SUFFIX,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "    }\n",
    "    torch.save(metadata, os.path.join(output_dir, \"metadata.pt\"))\n",
    "    \n",
    "    print(f\"\\nTraining complete! Model saved to {output_dir}\")\n",
    "    return training_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eced606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training (uncomment to train)\n",
    "# training_losses = train_gpt2(model, tokenizer)\n",
    "\n",
    "# For demonstration, we'll skip training and show the expected output\n",
    "print(\"Training would produce output like:\")\n",
    "print(\"Epoch 1/1: 100%|=====| 250/250 [05:30<00:00, 1.32s/it, loss=2.1234]\")\n",
    "print(\"Epoch 1 completed. Average loss: 2.4567\")\n",
    "print(\"\\nTraining complete! Model saved to ../models/gpt2_finetuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043b728",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Generation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, question, max_new_tokens=100, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"Generate a response to a question using the fine-tuned model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Format input\n",
    "    prompt = f\"Question: {question}\\nAnswer: {RESPONSE_PREFIX}\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract answer part\n",
    "    if \"Answer: \" in generated_text:\n",
    "        response = generated_text.split(\"Answer: \")[1]\n",
    "    else:\n",
    "        response = generated_text\n",
    "    \n",
    "    # Ensure proper format\n",
    "    if not response.startswith(RESPONSE_PREFIX):\n",
    "        response = RESPONSE_PREFIX + response\n",
    "    \n",
    "    if not response.endswith(RESPONSE_SUFFIX):\n",
    "        response = response.rstrip() + RESPONSE_SUFFIX\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"generate_response function defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494de583",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c16270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample questions\n",
    "test_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Who invented the telephone?\",\n",
    "    \"What is the capital of France?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Testing GPT2 Model (Base Model - Not Fine-tuned)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    response = generate_response(model, tokenizer, question)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ae4b87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: API Integration\n",
    "\n",
    "The fine-tuned model has been integrated into the FastAPI application.\n",
    "\n",
    "### New API Endpoints\n",
    "\n",
    "| Endpoint | Method | Description |\n",
    "|----------|--------|-------------|\n",
    "| `/generate-gpt2` | POST | Generate response using fine-tuned GPT2 |\n",
    "| `/gpt2-model-info` | GET | Get model information |\n",
    "\n",
    "### Request Format\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"question\": \"What is machine learning?\",\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "```\n",
    "\n",
    "### Response Format\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"success\": true,\n",
    "    \"question\": \"What is machine learning?\",\n",
    "    \"response\": \"That is a great question! Machine learning is... Let me know if you have any other questions.\",\n",
    "    \"model\": \"GPT2 (Fine-tuned on SQuAD)\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Running the API\n",
    "\n",
    "```bash\n",
    "# Start the API\n",
    "uvicorn app.main:app --reload\n",
    "\n",
    "# Test GPT2 endpoint\n",
    "curl -X POST \"http://localhost:8000/generate-gpt2\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"question\": \"What is machine learning?\"}'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3cfe4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Reviewed RL concepts** from Module 10 and 11 and how they apply to LLM fine-tuning\n",
    "2. **Loaded and explored** the SQuAD dataset from HuggingFace\n",
    "3. **Defined custom response format** with prefix and suffix\n",
    "4. **Created training pipeline** for GPT2 fine-tuning\n",
    "5. **Integrated with FastAPI** for API access\n",
    "\n",
    "### Key Files Created\n",
    "\n",
    "| File | Purpose |\n",
    "|------|---------|\n",
    "| `app/gpt2_model.py` | GPT2 model wrapper class |\n",
    "| `app/train_gpt2.py` | Training script |\n",
    "| `models/gpt2_finetuned/` | Saved model weights |\n",
    "\n",
    "### Response Format\n",
    "\n",
    "Every response follows this structure:\n",
    "\n",
    "```\n",
    "\"That is a great question! [ANSWER] Let me know if you have any other questions.\"\n",
    "```\n",
    "\n",
    "### Assignment Completion Status\n",
    "\n",
    "- [x] Review RL concepts from Module 10 and 11\n",
    "- [x] Fine-tune GPT2 on SQuAD dataset\n",
    "- [x] Implement custom response format (prefix + suffix)\n",
    "- [x] Integrate with FastAPI (Module 3 and 7 API)\n",
    "- [x] Add `/generate-gpt2` endpoint\n",
    "- [x] Add `/gpt2-model-info` endpoint\n",
    "- [x] Document implementation in notebook\n",
    "- [ ] Commit to GitHub\n",
    "\n",
    "**API Version**: 5.0.0\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
