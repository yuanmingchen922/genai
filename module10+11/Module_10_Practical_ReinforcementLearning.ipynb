{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Module 10: Practical - Basics of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "In the year 2147, Earthâ€™s surface has become a patchwork of unstable terrains after decades of climate decay.\n",
    "Our AI-controlled exploration unit â€” Bot-7 â€” is dispatched from its landing pod (ğŸ”µ) to reach a high-priority extraction point (ğŸ).\n",
    "\n",
    "The environment is treacherous and energy-limited. Bot-7 must autonomously learn the best route across a grid of dynamic terrain:\n",
    "\n",
    "ğŸŒ± Biofields: Soft, moss-covered terrain. Easy to traverse.\n",
    "\n",
    "ğŸŒŠ Flooded Zones: Shallow but energy-draining water channels.\n",
    "\n",
    "â›°ï¸ Crater Ridges: Volcanic rubble requiring intense power to cross.\n",
    "\n",
    "Each grid cell represents one unit of terrain. Moving across terrain consumes energy â€” some more than others.\n",
    "\n",
    "*Source: ChatGPT 4o (prompt: shortest path algorithm story with a sci-fi theme)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class GridGame:\n",
    "    TERRAIN_TYPES = {\n",
    "        \"ğŸŒ±\": 1,  # Land\n",
    "        \"ğŸŒŠ\": 3,  # Sea\n",
    "        \"â›°ï¸\": 5   # Mountain\n",
    "    }\n",
    "\n",
    "    def __init__(self, rows, cols, terrain_weights=None):\n",
    "        \"\"\"\n",
    "        :param rows: Number of rows in the grid\n",
    "        :param cols: Number of columns in the grid\n",
    "        :param terrain_weights: Optional weights for each terrain type\n",
    "        \"\"\"\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.terrain_symbols = list(self.TERRAIN_TYPES.keys())\n",
    "        self.terrain_costs = self.TERRAIN_TYPES\n",
    "\n",
    "        self.grid = self.generate_random_grid(rows, cols, terrain_weights)\n",
    "\n",
    "        self.cost_input_grid = {\n",
    "    (i,j): mo.ui.number(value = None) for i in range(rows) for j in range(cols) }\n",
    "\n",
    "        self.value_input_grid = {\n",
    "    (i,j): mo.ui.number(value = None) for i in range(rows) for j in range(cols) }\n",
    "\n",
    "        self.start, self.goal = self.pick_start_and_goal()\n",
    "        self.trajectory = []\n",
    "\n",
    "    def generate_random_grid(self, rows, cols, weights=None):\n",
    "        \"\"\"Generates a random terrain grid.\"\"\"\n",
    "        if weights is None:\n",
    "            weights = [1] * len(self.terrain_symbols)  # uniform if not provided\n",
    "        return [\n",
    "            [random.choices(self.terrain_symbols, weights=weights, k=1)[0] for _ in range(cols)]\n",
    "            for _ in range(rows)\n",
    "        ]\n",
    "\n",
    "    def pick_start_and_goal(self):\n",
    "        \"\"\"Randomly selects non-overlapping start and goal cells.\"\"\"\n",
    "        all_cells = [(i, j) for i in range(self.rows) for j in range(self.cols)]\n",
    "        start = random.choice(all_cells)\n",
    "        all_cells.remove(start)\n",
    "        goal = random.choice(all_cells)\n",
    "        return start, goal\n",
    "\n",
    "    def in_bounds(self, coord):\n",
    "        x, y = coord\n",
    "        return 0 <= x < self.rows and 0 <= y < self.cols\n",
    "\n",
    "    def get_cost(self, coord):\n",
    "        if not self.in_bounds(coord):\n",
    "            raise ValueError(f\"Coordinate {coord} is out of bounds.\")\n",
    "        terrain = self.grid[coord[0]][coord[1]]\n",
    "        return self.terrain_costs.get(terrain, float(\"inf\"))\n",
    "\n",
    "    def evaluate_trajectory(self, trajectory):\n",
    "        total_cost = 0\n",
    "        for coord in trajectory:\n",
    "            total_cost += self.get_cost(coord)\n",
    "            if coord == self.goal:\n",
    "                return total_cost, True\n",
    "        return total_cost, False\n",
    "\n",
    "    def evaluate_policy(self, policy, steps):\n",
    "        if not self.start:\n",
    "            raise ValueError(\"Start state must be defined.\")\n",
    "\n",
    "        actions = {\n",
    "            \"up\":    (-1, 0),\n",
    "            \"down\":  (1, 0),\n",
    "            \"left\":  (0, -1),\n",
    "            \"right\": (0, 1)\n",
    "        }\n",
    "\n",
    "        trajectory = []\n",
    "        current = self.start\n",
    "        total_cost = self.get_cost(current)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            if current == self.goal:\n",
    "                return total_cost, True\n",
    "\n",
    "            action = policy(current)\n",
    "            move = actions.get(action)\n",
    "            if move is None:\n",
    "                raise ValueError(f\"Invalid action '{action}' returned by policy.\")\n",
    "            next_state = (current[0] + move[0], current[1] + move[1])\n",
    "            if not self.in_bounds(next_state):\n",
    "                break\n",
    "            current = next_state\n",
    "            trajectory.append(current)\n",
    "            total_cost += self.get_cost(current)\n",
    "\n",
    "        return trajectory, total_cost, current == self.goal\n",
    "\n",
    "    def print_values(self, values, input_grid, test=False):\n",
    "        def color_style(value, color):\n",
    "            return f\"<span style='background-color:{color}; color:white; padding:2px 6px'>{value}</span>\"\n",
    "\n",
    "        def color_cell(index, value):\n",
    "            if (self.goal and index == self.goal):\n",
    "                return color_style(\"ğŸ\", \"black\")\n",
    "            if (self.start and index == self.start):\n",
    "                return color_style(\"ğŸ”µ\", \"blue\")  \n",
    "            if test:\n",
    "                return color_style(value, \"white\") + f\"{input_grid[index]}\"\n",
    "            else:\n",
    "                if input_grid[index].value == None:\n",
    "                    return \"\"\n",
    "                if input_grid[index].value == values[index]:\n",
    "                    return color_style( input_grid[index].value, \"green\")\n",
    "                else:\n",
    "                    return color_style( input_grid[index].value, \"red\")                \n",
    "        # Format rows\n",
    "        rows = []\n",
    "        for i, row in enumerate(self.grid):\n",
    "            formatted_row = [color_cell((i, j), val) for j, val in enumerate(row)]\n",
    "            rows.append(\"| \" + \" | \".join(formatted_row) + \" |\")\n",
    "\n",
    "        # Markdown table header separator\n",
    "        header_separator = \"| \" + \" | \".join([\"---\"] * len(self.grid[0])) + \" |\"\n",
    "        header = \"| \" + \" | \".join([\" \" for i in range(len(self.grid[0]))]) + \" |\"\n",
    "        # Insert header separator after first row\n",
    "        return \"\\n\".join([header, header_separator] + rows)\n",
    "\n",
    "    def print_grid(self, trajectory = None):\n",
    "        def color_style(value, color):\n",
    "            return f\"<span style='background-color:{color}; color:white; padding:2px 6px'>{value}</span>\"\n",
    "\n",
    "        def color_cell(index, value):\n",
    "            if (self.goal and index == self.goal):\n",
    "                return color_style(\"ğŸ\", \"black\")\n",
    "            if (self.start and index == self.start):\n",
    "                return color_style(\"ğŸ”µ\", \"blue\")  \n",
    "            if (trajectory and (index in trajectory)):\n",
    "                return color_style(value, \"green\")\n",
    "            else:\n",
    "                return color_style(value, \"white\")\n",
    "\n",
    "        # Format rows\n",
    "        rows = []\n",
    "        for i, row in enumerate(self.grid):\n",
    "            formatted_row = [color_cell((i, j), val) for j, val in enumerate(row)]\n",
    "            rows.append(\"| \" + \" | \".join(formatted_row) + \" |\")\n",
    "\n",
    "        # Markdown table header separator\n",
    "        header_separator = \"| \" + \" | \".join([\"---\"] * len(self.grid[0])) + \" |\"\n",
    "        header = \"| \" + \" | \".join([\" \" for i in range(len(self.grid[0]))]) + \" |\"\n",
    "        # Insert header separator after first row\n",
    "        return \"\\n\".join([header, header_separator] + rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "If the terrain map is known, Bot-7 can compute the most energy-efficient route to the target using principles of **dynamic programming** â€” evaluating each cell's cumulative cost from goal to start and choosing the least expensive path. In particular, let's assign the following costs to going through each type of terrain: \"ğŸŒ±\": 1, \"ğŸŒŠ\": 3, \"â›°ï¸\": 5. Below is a solution to this problem using a very well known algorithm known as Dijkstra's Algorithm (feel free to look at the code if you are interested, but in reinforcement learning our goal will be to understand how to do this without knowing the map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "# Note this is an artificial example that calculates the exact costs and rewards to be used in the remainder of the notebook for demonstration purposes. It combines both cost based calculations and reward value function calculation instead of relying on only one of the approaches.\n",
    "\n",
    "def dijkstra(grid, terrain_costs, start, goal):\n",
    "    \"\"\"\n",
    "    Finds the lowest-cost path using Dijkstra's algorithm.\n",
    "\n",
    "    :param grid: 2D list of terrain symbols (e.g. [[\"ğŸŒ±\", \"ğŸŒŠ\", \"â›°ï¸\"]])\n",
    "    :param terrain_costs: dict mapping symbols to cost (e.g. {\"ğŸŒ±\": 1, \"ğŸŒŠ\": 3, \"â›°ï¸\": 5})\n",
    "    :param start: (row, col) tuple\n",
    "    :param goal: (row, col) tuple\n",
    "    :return: (total_cost, path as list of (row, col))\n",
    "    \"\"\"\n",
    "    rows, cols = len(grid), len(grid[0])\n",
    "\n",
    "    def in_bounds(coord):\n",
    "        x, y = coord\n",
    "        return 0 <= x < rows and 0 <= y < cols\n",
    "\n",
    "    def get_cost(coord):\n",
    "        x, y = coord\n",
    "        return terrain_costs.get(grid[x][y], float(\"inf\"))\n",
    "\n",
    "    def get_neighbors(coord):\n",
    "        x, y = coord\n",
    "        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "        return [(x + dx, y + dy) for dx, dy in directions if in_bounds((x + dx, y + dy))]\n",
    "\n",
    "    visited = set()\n",
    "    heap = [(0, goal, [goal])]\n",
    "\n",
    "    costs = { (i, j): float('inf') for i in range(rows) for j in range(cols) }\n",
    "    costs[goal] = 0\n",
    "\n",
    "    path_to_goal = []\n",
    "    cost_to_goal = float(\"inf\")\n",
    "\n",
    "    while heap:\n",
    "        cost, current, path = heapq.heappop(heap)\n",
    "        if current in visited:\n",
    "            continue\n",
    "        visited.add(current)\n",
    "\n",
    "        if current == start:\n",
    "            path_to_goal = path\n",
    "            cost_to_goal = cost\n",
    "\n",
    "        for neighbor in get_neighbors(current):\n",
    "            new_cost = cost + get_cost(neighbor)\n",
    "            if new_cost < costs[neighbor]:\n",
    "                costs[neighbor] = new_cost\n",
    "                heapq.heappush(heap, (new_cost, neighbor, [neighbor] + path))\n",
    "\n",
    "    return cost_to_goal, path_to_goal, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Cost: 10\n",
      "Trajectory: [(0, 2), (0, 1), (0, 0), (1, 0), (2, 0), (3, 0), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "game = GridGame(rows=5, cols=5)\n",
    "\n",
    "cost, trajectory, costs = dijkstra(game.grid, game.terrain_costs, game.start, game.goal)\n",
    "print(f\"\\nTotal Cost: {cost}\")\n",
    "print(f\"Trajectory: {trajectory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|   |   |   |   |   |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:blue; color:white; padding:2px 6px'>ğŸ”µ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:black; color:white; padding:2px 6px'>ğŸ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.md(f\"\"\"{game.print_grid()}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|   |   |   |   |   |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| <span style='background-color:green; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:green; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:blue; color:white; padding:2px 6px'>ğŸ”µ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> |\n",
       "| <span style='background-color:green; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> |\n",
       "| <span style='background-color:green; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> |\n",
       "| <span style='background-color:green; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:black; color:white; padding:2px 6px'>ğŸ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.md(f\"\"\"{game.print_grid(trajectory)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Dijkstra's algorithm (and more generally dynamic programming algorithms) rely on a simple intution, which will also be very useful to us later with RL.  From each cell in the grid there is a smallest cost path to the goal (not necessarily unique). The expected cost of this path is sometimes referred to as the **cost-to-go** of that state. Let's try to understand how to calculate the value function. Fill in the table below with what you think the cost of the optimal path will be (smallest total cost to the goal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|   |   |   |   |   |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-0' random-id='77f66be3-24d8-9f3c-53c6-8de176142907'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span><marimo-ui-element object-id='SFPL-1' random-id='dd343a64-98a4-fbb7-aace-e360086ea69a'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:blue; color:white; padding:2px 6px'>ğŸ”µ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-3' random-id='6b654d77-47e1-010f-a663-24120e18e419'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-4' random-id='a5048e4c-f65f-a8b4-2f89-82bb2df937df'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-5' random-id='2136b67b-5658-187c-3864-06f61d9250e9'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-6' random-id='fad4acfe-8ff3-2816-f008-ae729ad8f66c'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span><marimo-ui-element object-id='SFPL-7' random-id='44646187-3d57-d390-5b6d-981bbbe968c9'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-8' random-id='e9a4a880-a16b-23f4-7b8f-255a4ffa19c3'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-9' random-id='de985b1f-dc91-fc6b-8d40-e45e24e87264'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-10' random-id='e5b4887d-60d7-239a-1750-6cc5a5dee50b'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-11' random-id='431e4898-fc46-c5d2-3668-15354826d430'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-12' random-id='0062487a-5263-e100-19eb-45cf2aa89e96'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-13' random-id='dcac486f-a670-4b21-b90e-1429a2cea12e'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span><marimo-ui-element object-id='SFPL-14' random-id='8a8b0e6f-9e74-b98e-b8e2-5a013d81f2f2'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span><marimo-ui-element object-id='SFPL-15' random-id='0a282b6d-fe93-d753-5460-432dc34e3d15'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:black; color:white; padding:2px 6px'>ğŸ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-17' random-id='bb3bc163-16a0-3e92-52a4-d594779b4ec0'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span><marimo-ui-element object-id='SFPL-18' random-id='de0482b6-6b9d-a7d4-36c4-58dba345f3d5'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-19' random-id='6b91971a-5850-7e07-a3c4-aa1d73fca831'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span><marimo-ui-element object-id='SFPL-20' random-id='4c474a64-d827-e1e4-f713-dc2fab788710'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-21' random-id='00882298-d8e8-8e42-4280-2dea53574774'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-22' random-id='a51fae47-ffa1-3fc8-7131-bed346433d18'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-23' random-id='77b0e461-1d1f-098b-8e5f-57643c3fedc0'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-24' random-id='df532946-5cb9-bff0-6b1f-ca6ab46764a2'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.md(f\"\"\"{game.print_values(costs, game.cost_input_grid, test=True)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "&lt;marimo-button data-initial-value=&#x27;0&#x27; data-label=&#x27;&amp;quot;&amp;lt;span class=&amp;#92;&amp;quot;markdown prose dark:prose-invert&amp;#92;&amp;quot;&amp;gt;&amp;lt;span class=&amp;#92;&amp;quot;paragraph&amp;#92;&amp;quot;&amp;gt;Verify Optimal Costs&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;quot;&#x27; data-kind=&#x27;&amp;quot;success&amp;quot;&#x27; data-disabled=&#x27;false&#x27; data-full-width=&#x27;false&#x27;&gt;&lt;/marimo-button&gt;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "button = mo.ui.button(label=\"Verify Optimal Costs\", value=\"\", kind=\"success\", on_click= lambda value : game.print_values(costs, game.cost_input_grid))\n",
    "button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.md(f\"\"\"{button.value}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Before we proceed further, weâ€™ll make one important conceptual shift. From this point on, we will refer not to costs, but to rewards.\n",
    "\n",
    "\n",
    "- In reinforcement learning, agents are trained to maximize reward rather than minimize cost.\n",
    "\n",
    "- Mathematically, the two are equivalent: *reward* = âˆ’*cost*\n",
    "\n",
    "So when Bot-7 is faced with a grid of terrain, each movement will now yield a (negative) reward, representing energy loss. Its mission becomes one of **maximizing total reward**, which naturally leads to minimizing total energy use.\n",
    "\n",
    "This reward-based framing aligns with how most RL algorithms are formulated, and sets the stage for what comes next: value functions, policies, and learning through experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLit",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's now frame the problem as an RL problem and understand better the terms: **agent**, **environment**, **state**, **action**, **reward** as applied to this example.\n",
    "\n",
    "The **agent** is simply our Bot-7 (or its decision making system). Its **state** is its position in our grid matrix and can be described with coordinates (i,j) representing the row and column of the position. The **actions** available to Bot-7 are moving up, down, left, or right (except when it is at the boundary, then only some of the actions are available).\n",
    "\n",
    "The **environment** abstracts all the complexities of putting the agent in the next state based on its action decision. This could be the robot actuators, the effects of wind and anything else.  For this example we'll assume a deterministic environment that simply maps the current state and action to the next state:\n",
    "\n",
    "$$s(t+1) = f(s(t), a(t))$$\n",
    "\n",
    "by moving the robot to up, down, left, or right based on the action $a(t)$.\n",
    "\n",
    "Note: to make things more interesting we could introduce a stochastic environment that, for example, moves the bot to the state corresponding to the action with 80% probability, and moves it to one of the other neighboring states with a 20% probability.\n",
    "\n",
    "Finally, along with updating the state, the environment also assigns some **reward** (in our case corresponding to one of the terrain types the bot ended up in or an extra reward if it reached the goal)\n",
    "\n",
    "Rewards:\n",
    "\n",
    "- ğŸŒ±: -1\n",
    "- ğŸŒŠ: -3\n",
    "- â›°ï¸: -5\n",
    "\n",
    "Recall that in RL the agent usually does not have access to the map and doesn't know in advance which cells will give which rewards. It will have to discover it by trying various actions in an efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "For now, let's pretend again that we know what the map looks like:\n",
       "\n",
       "|   |   |   |   |   |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:blue; color:white; padding:2px 6px'>ğŸ”µ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:black; color:white; padding:2px 6px'>ğŸ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> |\n",
       "\n",
       "The agent needs some way to make decisions based on the information it has. This is called a **policy**. Let's assume the agent's policy is to first move vertically to get to the same column as the goal and then move horizontally. Let's evaluate this policy to calculate the total amount of reward the agent would get."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.md(\n",
    "    f\"\"\"\n",
    "For now, let's pretend again that we know what the map looks like:\n",
    "\n",
    "{game.print_grid()}\n",
    "\n",
    "The agent needs some way to make decisions based on the information it has. This is called a **policy**. Let's assume the agent's policy is to first move vertically to get to the same column as the goal and then move horizontally. Let's evaluate this policy to calculate the total amount of reward the agent would get.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple policy returning the direction to move \n",
    "# horizontally (+-1, 0) or vertically (0, +-1)\n",
    "def simple_policy(state):\n",
    "    x, y = state\n",
    "    gx, gy = game.goal\n",
    "    if (x < gx):\n",
    "        return (1, 0)\n",
    "    if (x > gx):\n",
    "        return (-1, 0)\n",
    "    if (y < gy):\n",
    "        return (0, 1)\n",
    "    if (y > gy):\n",
    "        return (0, -1)\n",
    "\n",
    "# Function to get the reward of moving to a new cell\n",
    "# This is part of the environment and is used only\n",
    "# to evaluate the policy, not to learn it.\n",
    "def get_reward(x, y):\n",
    "    return -game.terrain_costs.get(game.grid[x][y])\n",
    "\n",
    "# Function to take a step in the environment\n",
    "# This simulates the environment's response to the agent's action\n",
    "# and returns the new state and the reward\n",
    "def step(state, action):\n",
    "    x, y = state\n",
    "    dx, dy = action\n",
    "    nx, ny = x + dx, y + dy\n",
    "    return (nx, ny), get_reward(nx, ny)\n",
    "\n",
    "# Function to evaluate the policy by simulating the agent's actions\n",
    "# based on the policy we define, in this case: simple_policy\n",
    "def evaluate_policy():\n",
    "    state = game.start\n",
    "    trajectory = []\n",
    "    total_reward = 0\n",
    "    while state != game.goal:\n",
    "        action = simple_policy(state)\n",
    "        state, reward = step(state, action)\n",
    "        if state != game.goal:\n",
    "            total_reward += reward\n",
    "            trajectory.append(state)\n",
    "    print(f\"\\nTotal reward: {total_reward}\")\n",
    "    print(f\"Trajectory: {trajectory}\")\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Experiment with the simple_policy function definition above and the corresponding rewards from the policy **rollouts** (or **trajectories** resulting from following the policy.)\n",
       "\n",
       "<marimo-ui-element object-id='qnkX-0' random-id='10f06f5c-934e-32c6-307b-3dab425440a9'><marimo-button data-initial-value='0' data-label='&quot;&lt;span class=&#92;&quot;markdown prose dark:prose-invert&#92;&quot;&gt;&lt;span class=&#92;&quot;paragraph&#92;&quot;&gt;Evaluate the simple_policy&lt;/span&gt;&lt;/span&gt;&quot;' data-kind='&quot;success&quot;' data-disabled='false' data-full-width='false'></marimo-button></marimo-ui-element>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy_eval_button = mo.ui.button(label=\"Evaluate the simple_policy\", value=[], kind=\"success\", on_click= lambda value : evaluate_policy())\n",
    "\n",
    "mo.md(f'''\n",
    "Experiment with the simple_policy function definition above and the corresponding rewards from the policy **rollouts** (or **trajectories** resulting from following the policy.)\n",
    "\n",
    "{policy_eval_button}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|   |   |   |   |   |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:blue; color:white; padding:2px 6px'>ğŸ”µ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:black; color:white; padding:2px 6px'>ğŸ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.md(f\"\"\"{game.print_grid(policy_eval_button.value)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vxnm",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "As we see, following different policies results in the different rewards at the end of the  **episode** (note how we use the terms rollout, trajectory, episode interchangeably). The goal in RL is to find optimal policies resulting in largest possible reward for the agent.\n",
    "\n",
    "While learning the algorithms for finding such policies is outside the scope of this course, most popular methods rely on using some function approximators (like neural networks) to either learn the **value** function or the policy directly.\n",
    "\n",
    "Value functions are analogous to the optimal cost to the goal that we saw in the beginning of this notebook. Instead of giving the cost of the optimal path from each state, value functions give the total reward (or expected reward in the stochastic case) that the agent would gain by either following a specific policy (policy value function) or by following the optimal policy. To get more intution behind value functions we'll fill in the optimal value table analogous to the cost to the goal table.\n",
    "\n",
    "As you fill in this table refer to the slides and make sure you understand how these optimal values satisfy the Bellman Equation for the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "rewards = {}\n",
    "for key in costs:\n",
    "    rewards[key] = - costs[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|   |   |   |   |   |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-25' random-id='36e7030c-fb79-05fc-62b9-cac37de73977'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span><marimo-ui-element object-id='SFPL-26' random-id='5ae277ca-da3b-c130-bf6d-023f9d95f4d1'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:blue; color:white; padding:2px 6px'>ğŸ”µ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-28' random-id='e7f885d9-ae95-df75-5709-e0ce909510f6'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-29' random-id='951480f4-a827-95ea-787d-bb2ced0a125e'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-30' random-id='b3595722-6663-bd67-1cff-a5f937917a0b'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-31' random-id='40f1fdca-1c6a-57c8-f9e8-1cea315dedc7'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span><marimo-ui-element object-id='SFPL-32' random-id='999cb649-dc0e-4cfe-65c3-f3c8624264d1'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-33' random-id='5e97df27-dab1-f3d5-aeaf-9b1c28da1399'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-34' random-id='3f443a8a-c3df-41f6-b98b-af979cba58d8'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-35' random-id='1f374a67-98b7-297c-8bc1-93360f49956d'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-36' random-id='537a144d-35dd-7361-3519-57f5065a8463'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-37' random-id='3cf2d6a0-8127-1ced-e90f-1488c4c3709a'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-38' random-id='0951236a-e106-1203-49b6-4ca5ffbb6917'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span><marimo-ui-element object-id='SFPL-39' random-id='ab873250-211c-1705-777e-1e85daf202e6'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span><marimo-ui-element object-id='SFPL-40' random-id='cac1179d-26e6-b0bf-63dd-e39cee1727f0'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:black; color:white; padding:2px 6px'>ğŸ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-42' random-id='f14d0d4d-b887-6559-07c1-cbbd00ed54a2'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span><marimo-ui-element object-id='SFPL-43' random-id='703e6d3b-7afa-2701-b480-85ee7bc5e233'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span><marimo-ui-element object-id='SFPL-44' random-id='cfda9b21-13f8-b076-cff2-7c814f435ec3'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span><marimo-ui-element object-id='SFPL-45' random-id='80e9957f-a674-1bb7-38f4-33c3c31ca44b'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-46' random-id='fad3a239-133b-5134-f195-14f975de90fa'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-47' random-id='4d341bc7-8ba7-8b58-8fa2-8845bd6369c3'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-48' random-id='67ece9bd-d8df-49fa-8b0b-e7b3659507ce'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span><marimo-ui-element object-id='SFPL-49' random-id='c01fc28f-0502-62f8-9eb3-9422d6dd2bda'><marimo-number data-initial-value='null' data-label='null' data-debounce='false' data-full-width='false' data-disabled='false'></marimo-number></marimo-ui-element> |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.md(f\"\"\"{game.print_values(rewards, game.value_input_grid, test=True)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "&lt;marimo-button data-initial-value=&#x27;0&#x27; data-label=&#x27;&amp;quot;&amp;lt;span class=&amp;#92;&amp;quot;markdown prose dark:prose-invert&amp;#92;&amp;quot;&amp;gt;&amp;lt;span class=&amp;#92;&amp;quot;paragraph&amp;#92;&amp;quot;&amp;gt;Verify the Value Function&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;quot;&#x27; data-kind=&#x27;&amp;quot;success&amp;quot;&#x27; data-disabled=&#x27;false&#x27; data-full-width=&#x27;false&#x27;&gt;&lt;/marimo-button&gt;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "values_button = mo.ui.button(label=\"Verify the Value Function\", value=\"\", kind=\"success\", on_click= lambda value : game.print_values(rewards, game.value_input_grid))\n",
    "values_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.md(f\"\"\"{values_button.value}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "In control problems, where the goal is to come up with an optimal policy, a more useful function is the **q-function**, which is analogous to the value function, but keeps track of the total expected reward based on not only the current state, but also the current action. Some of the RL algorithms use function approximators like neural networks to **learn** the value/q function (remember that in real scenario you don't have a map so cannot do the calculation above) by trying or observing many episodes or alternating policy improvements and q-function approximations under current policy.\n",
    "\n",
    "Once you know the optimal value/q functions figuring out a policy is straight-forward, as the agent takes the action with the maximum expected reward (given by value/q functions). This is referred to as 'acting greedily' with respect to it.\n",
    "\n",
    "Another common approach is to forego the intermediate step of learning the value function and instead learn directly the policy mapping $a(t) = \\pi(s(t))$ from states to actions (this is particularly useful in the stochastic case when a neural network with a final softmax activation layer can give the probabilities of taking each action $a_i$ based on the current state $s(t)$,\n",
    "\n",
    "$$\n",
    "\\pi(a_i(t) | s(t)).\n",
    "$$\n",
    "\n",
    "We finish with an example of one such method. Understanding the details of this and other methods is outside the scope of this course, but feel free to explore and play around with it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gradient(grid, terrain_costs, start, goal, episodes=1000, gamma=0.99, lr=0.001):\n",
    "    rows, cols = len(grid), len(grid[0])\n",
    "    actions = ['up', 'down', 'left', 'right']\n",
    "    action_to_delta = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
    "    n_actions = len(actions)\n",
    "\n",
    "    # Initialize policy weights Î¸[state][action]\n",
    "    theta = np.zeros((rows, cols, n_actions))\n",
    "\n",
    "    def in_bounds(x, y):\n",
    "        return 0 <= x < rows and 0 <= y < cols\n",
    "\n",
    "    def get_reward(x, y):\n",
    "        return -terrain_costs.get(grid[x][y], -float('inf'))\n",
    "\n",
    "    def softmax(logits):\n",
    "        exps = np.exp(logits - np.max(logits))  # for stability\n",
    "        return exps / np.sum(exps)\n",
    "\n",
    "    def choose_action(state):\n",
    "        x, y = state\n",
    "        probs = softmax(theta[x][y])\n",
    "        action_index = np.random.choice(range(n_actions), p=probs)\n",
    "        return actions[action_index], action_index, probs\n",
    "\n",
    "    def step(state, action_name):\n",
    "        dx, dy = action_to_delta[action_name]\n",
    "        x, y = state\n",
    "        nx, ny = x + dx, y + dy\n",
    "        if in_bounds(nx, ny):\n",
    "            return (nx, ny), get_reward(nx, ny)\n",
    "        return (x, y), get_reward(x, y)  # no-op with penalty\n",
    "\n",
    "    # Training loop\n",
    "    for episode in range(episodes):\n",
    "        state = start\n",
    "        trajectory = []\n",
    "        for _ in range(100):  # max steps per episode\n",
    "            action_name, action_idx, probs = choose_action(state)\n",
    "            next_state, reward = step(state, action_name)\n",
    "            trajectory.append((state, action_idx, reward))\n",
    "            state = next_state\n",
    "            if state == goal:\n",
    "                break\n",
    "\n",
    "        # Compute returns and update Î¸\n",
    "        G = 0\n",
    "        for t in reversed(range(len(trajectory))):\n",
    "            state, action_idx, reward = trajectory[t]\n",
    "            G = reward + gamma * G\n",
    "            x, y = state\n",
    "            probs = softmax(theta[x][y])\n",
    "            grad = -probs\n",
    "            grad[action_idx] += 1  # âˆ‡log Ï€\n",
    "            theta[x][y] += lr * G * grad  # policy gradient step\n",
    "\n",
    "    # Generate best path using learned policy\n",
    "    path = [start]\n",
    "    state = start\n",
    "    visited = set()\n",
    "    for _ in range(100):\n",
    "        x, y = state\n",
    "        if state == goal or state in visited:\n",
    "            break\n",
    "        visited.add(state)\n",
    "        probs = softmax(theta[x][y])\n",
    "        action_name = actions[np.argmax(probs)]\n",
    "        state, _ = step(state, action_name)\n",
    "        path.append(state)\n",
    "\n",
    "    total_cost = sum(get_reward(x, y) for x, y in path[1:-1])\n",
    "    return total_cost, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy Gradient Cost: -9\n",
      "Path: [(0, 2), (0, 1), (1, 1), (2, 1), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "p_cost, p_trajectory = policy_gradient(game.grid, game.terrain_costs, game.start, game.goal, 10000)\n",
    "print(f\"\\nPolicy Gradient Cost: {p_cost}\")\n",
    "print(f\"Path: {p_trajectory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|   |   |   |   |   |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:green; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:blue; color:white; padding:2px 6px'>ğŸ”µ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:green; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:green; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:black; color:white; padding:2px 6px'>ğŸ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒ±</span> |\n",
       "| <span style='background-color:white; color:white; padding:2px 6px'>ğŸŒŠ</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> | <span style='background-color:white; color:white; padding:2px 6px'>â›°ï¸</span> |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.md(f\"\"\"{game.print_grid(p_trajectory)}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
